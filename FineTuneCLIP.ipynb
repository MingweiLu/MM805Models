{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mingwei/.local/lib/python3.8/site-packages (2.2.1)\n",
      "Requirement already satisfied: pillow in /home/mingwei/anaconda3/envs/clip/lib/python3.8/site-packages (9.3.0)\n",
      "Requirement already satisfied: filelock in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mingwei/anaconda3/envs/clip/lib/python3.8/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/mingwei/anaconda3/envs/clip/lib/python3.8/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/mingwei/.local/lib/python3.8/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mingwei/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mingwei/anaconda3/envs/clip/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mingwei/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (77) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(images)\n\u001b[0;32m---> 48\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m@\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcardboard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplastic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits_per_image, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/clip/lib/python3.8/site-packages/clip/model.py:346\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    344\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(text)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (77) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = 'garbage classification/Garbage classification'\n",
    "\n",
    "# Load the CLIP model and its preprocessing tools\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Custom dataset using ImageFolder\n",
    "dataset = ImageFolder(root=dataset_path, transform=preprocess)\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n",
    "\n",
    "# Specify the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for images, labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        image_features = model.encode_image(images)\n",
    "        logits_per_image = image_features @ model.encode_text(clip.tokenize([\"glass\", \"cardboard\", \"metal\", \"paper\", \"plastic\"] * (labels.shape[0] // 5 + 1)).to(device)[:labels.shape[0]].T)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits_per_image, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        if device != \"cpu\":\n",
    "            convert_models_to_fp32(model)\n",
    "\n",
    "# Note: Adjust the tokenization part to your specific needs, it assumes each batch is evenly divisible by number of classes which may not be the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/30, Loss: 2.6543: 100%|██████████| 79/79 [00:21<00:00,  3.72it/s]\n",
      "Epoch 1/30, Loss: 2.2734: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 2/30, Loss: 3.1055: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 3/30, Loss: 2.5547: 100%|██████████| 79/79 [00:20<00:00,  3.83it/s]\n",
      "Epoch 4/30, Loss: 2.7461: 100%|██████████| 79/79 [00:20<00:00,  3.92it/s]\n",
      "Epoch 5/30, Loss: 2.7539: 100%|██████████| 79/79 [00:20<00:00,  3.85it/s]\n",
      "Epoch 6/30, Loss: 2.6719: 100%|██████████| 79/79 [00:20<00:00,  3.79it/s]\n",
      "Epoch 7/30, Loss: 2.9922: 100%|██████████| 79/79 [00:20<00:00,  3.92it/s]\n",
      "Epoch 8/30, Loss: 2.6992: 100%|██████████| 79/79 [00:20<00:00,  3.80it/s]\n",
      "Epoch 9/30, Loss: 2.6855: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 10/30, Loss: 2.7656: 100%|██████████| 79/79 [00:21<00:00,  3.75it/s]\n",
      "Epoch 11/30, Loss: 2.4688: 100%|██████████| 79/79 [00:20<00:00,  3.78it/s]\n",
      "Epoch 12/30, Loss: 3.1699: 100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n",
      "Epoch 13/30, Loss: 2.8047: 100%|██████████| 79/79 [00:20<00:00,  3.78it/s]\n",
      "Epoch 14/30, Loss: 2.9336: 100%|██████████| 79/79 [00:20<00:00,  3.83it/s]\n",
      "Epoch 15/30, Loss: 2.8281: 100%|██████████| 79/79 [00:20<00:00,  3.83it/s]\n",
      "Epoch 16/30, Loss: 2.6719: 100%|██████████| 79/79 [00:21<00:00,  3.75it/s]\n",
      "Epoch 17/30, Loss: 2.8223: 100%|██████████| 79/79 [00:20<00:00,  3.83it/s]\n",
      "Epoch 18/30, Loss: 2.6816: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 19/30, Loss: 2.6328: 100%|██████████| 79/79 [00:20<00:00,  3.88it/s]\n",
      "Epoch 20/30, Loss: 2.5391: 100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n",
      "Epoch 21/30, Loss: 2.2969: 100%|██████████| 79/79 [00:20<00:00,  3.85it/s]\n",
      "Epoch 22/30, Loss: 2.4785: 100%|██████████| 79/79 [00:20<00:00,  3.78it/s]\n",
      "Epoch 23/30, Loss: 2.6211: 100%|██████████| 79/79 [00:20<00:00,  3.86it/s]\n",
      "Epoch 24/30, Loss: 2.6426: 100%|██████████| 79/79 [00:20<00:00,  3.86it/s]\n",
      "Epoch 25/30, Loss: 2.6133: 100%|██████████| 79/79 [00:20<00:00,  3.77it/s]\n",
      "Epoch 26/30, Loss: 2.6113: 100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n",
      "Epoch 27/30, Loss: 2.5605: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 28/30, Loss: 2.2871: 100%|██████████| 79/79 [00:20<00:00,  3.81it/s]\n",
      "Epoch 29/30, Loss: 2.3457: 100%|██████████| 79/79 [00:20<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = 'garbage classification/Garbage classification'\n",
    "\n",
    "# Choose computation device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Prepare image paths and labels\n",
    "list_image_path = []\n",
    "list_txt = []\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_dir = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for img_filename in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_filename)\n",
    "            list_image_path.append(img_path)\n",
    "            list_txt.append(class_name)  # Use the folder name as the label\n",
    "\n",
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path, list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(Image.open(self.image_path[idx]))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed\n",
    "\n",
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch \n",
    "        \n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load pre-trained CLIP model (make sure it includes your custom trained weights if applicable)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Ensure your model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Preprocess the image\n",
    "    image_preprocessed = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image_preprocessed\n",
    "\n",
    "def predict_image_class(image_path, model, device):\n",
    "    # Prepare the image\n",
    "    image_tensor = prepare_image(image_path).to(device)\n",
    "\n",
    "    # Define class names (ensure these are the same as used during training)\n",
    "    class_names = [\"This is a picture of glass garbage\", \"This is a picture of cardboard garbage\", \"This is a picture of metal garbage\", \"This is a picture of paper garbage\", \"This is a picture of plastic garbage\"]\n",
    "    text_tokens = clip.tokenize(class_names).to(device)\n",
    "\n",
    "    # Generate image and text features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # Calculate the similarity (dot product) between image features and text features\n",
    "    logits = image_features @ text_features.T\n",
    "    probabilities = logits.softmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Get the top prediction\n",
    "    top_class_index = probabilities.argmax()\n",
    "    return class_names[top_class_index], probabilities[0, top_class_index]\n",
    "\n",
    "\n",
    "image_path = '/home/mingwei/Desktop/CLIP/garbage classification/Garbage classification/glass/glass1.jpg'\n",
    "predicted_class, confidence = predict_image_class(image_path, model, device)\n",
    "print(f\"Predicted class: {predicted_class} with confidence {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image paths and labels for the test set\n",
    "test_dataset_path = 'garbage classification/Garbage classification'\n",
    "list_test_image_path = []\n",
    "list_test_txt = []\n",
    "for class_name in os.listdir(test_dataset_path):\n",
    "    class_dir = os.path.join(test_dataset_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for img_filename in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_filename)\n",
    "            list_test_image_path.append(img_path)\n",
    "            list_test_txt.append(class_name)  # Use the folder name as the label\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = image_title_dataset(list_test_image_path, list_test_txt)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # Use non-shuffling loader for testing\n",
    "\n",
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for images, titles in dataloader:\n",
    "            images = images.to(device)\n",
    "            titles = titles.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_per_image, _ = model(images, titles)\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            predicted = logits_per_image.argmax(dim=1)\n",
    "            \n",
    "            # Compare with ground truth\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            correct += (predicted == ground_truth).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Calculate the accuracy on the test dataset\n",
    "test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
